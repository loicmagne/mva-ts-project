{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "\n",
    "- Implement LGS ✅\n",
    "- Implement VSGS ✅\n",
    "- Explore multiple dataset generation approach ✅ (Just scenario 3 left, I am not sure if it is important)\n",
    "- Show results ✅\n",
    "- Implement PSD estimation algorithm (currently the true PSD is used) ✅\n",
    "- Compare the \"graph baseline\" with traditional kernel methods ~ need some work to find penalty parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygsp\n",
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ruptures.metrics import *\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import gamma\n",
    "\n",
    "SEED = 8976813\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(true_bkps, bkps, verbose=True):\n",
    "    p, r = rpt.metrics.precision_recall(true_bkps, bkps)\n",
    "    hausdorff = rpt.metrics.hausdorff(true_bkps, bkps)\n",
    "    randindex = rpt.metrics.randindex(true_bkps, bkps)\n",
    "    hamming = rpt.metrics.hamming(true_bkps, bkps)\n",
    "    if verbose:\n",
    "        print(f\"\"\"\n",
    "            Hausdorff: {hausdorff:.2f}, \n",
    "            Precision: {p:.2f}, \n",
    "            Recall: {r:.2f}, \n",
    "            Randindex: {randindex:.2f}, \n",
    "            Hamming: {hamming:.2f}\n",
    "        \"\"\")\n",
    "    return p, r, hausdorff, randindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_barabasi_albert(\n",
    "        n_nodes,\n",
    "        m_barabasi=4,\n",
    "        mean_exponential=20,\n",
    "        random_freq=20\n",
    "    ):\n",
    "    # Generate Graph Structure\n",
    "    G = pygsp.graphs.BarabasiAlbert(N=n_nodes, m0=m_barabasi, m=m_barabasi, seed=SEED)\n",
    "    G.set_coordinates()\n",
    "    G.compute_fourier_basis()\n",
    "\n",
    "    spectral_profile=lambda x: 2*gamma.pdf(x,a=20.,loc=5.)+1.0\n",
    "    H = pygsp.filters.Filter(G, lambda x: spectral_profile(x))\n",
    "    PSD = (spectral_profile(G.e))**2\n",
    "\n",
    "    # Generate Signals\n",
    "    n_bkps = 4\n",
    "    bkps = []\n",
    "    mus = []\n",
    "    signals = []\n",
    "    \n",
    "    acc_bkp = 0\n",
    "    acc_mean = np.concatenate((np.random.uniform(-5.0,5.0,size=random_freq),np.zeros(n_nodes-random_freq)))\n",
    "    acc_mean = G.igft(acc_mean).reshape(-1, 1)\n",
    "    # Make a list of changing points after each breakpoints\n",
    "    changing_nodes = []\n",
    "    changing_nodes.append(tuple([[]])) # In the first part, no mean are changed\n",
    "    max_d = np.argmax(G.d)\n",
    "    max_d_neighbors = np.where(G.W.todense()[max_d] != 0)[1]\n",
    "    changing_nodes.append(np.concatenate([max_d_neighbors, [max_d]])) # After the first break, node with maximum degree and neighbord change\n",
    "    changing_nodes.append(np.argpartition(G.d, -5)[-5:]) # After the second break, 5 nodes with highest degree change\n",
    "    changing_nodes.append(np.random.choice(range(n_nodes), size=random_freq, replace=False)) # After last break, 20 random nodes are changes\n",
    "\n",
    "    for i in range(n_bkps):\n",
    "        # Generate segment\n",
    "        segment_length = int(np.random.exponential(scale=mean_exponential)+30)\n",
    "        acc_bkp += segment_length\n",
    "        acc_mean[changing_nodes[i]] = np.random.uniform(-5.0,5.0,size=(len(changing_nodes[i]),1)) # Changing mean of corresponding nodes\n",
    "        signal = np.random.normal(size=(n_nodes,segment_length))\n",
    "        signal = H.filter(signal) + acc_mean\n",
    "\n",
    "        # Record bkp position, mean and signal\n",
    "        bkps.append(acc_bkp)\n",
    "        mus.append(np.copy(acc_mean))\n",
    "        signals.append(signal)\n",
    "\n",
    "    signal = np.concatenate(signals, 1)\n",
    "    return G, PSD, bkps, mus, signal\n",
    "\n",
    "# Params\n",
    "n_nodes = 500   \n",
    "G, PSD, true_bkps, mus, signal = generate_barabasi_albert(n_nodes)\n",
    "print(f'{len(true_bkps)} changes points')\n",
    "rpt.display(signal[:5].T, true_bkps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_erdos_renyi(\n",
    "        n_nodes,\n",
    "        p=0.3,\n",
    "        mean_exponential=20,\n",
    "        mean_change_points=3,\n",
    "        random_freq=20\n",
    "    ):\n",
    "    # Generate Graph Structure\n",
    "    G = pygsp.graphs.ErdosRenyi(N=n_nodes, p=p, seed=SEED)\n",
    "    G.set_coordinates()\n",
    "    G.compute_fourier_basis()\n",
    "\n",
    "    spectral_profile = lambda x: np.sqrt(15)/(np.log(x+10)+1)\n",
    "    H = pygsp.filters.Filter(G, lambda x: spectral_profile(x))\n",
    "    PSD = (spectral_profile(G.e))**2\n",
    "\n",
    "    # Generate Signals\n",
    "    n_bkps = np.random.poisson(lam=mean_change_points) + 2\n",
    "    bkps = []\n",
    "    mus = []\n",
    "    signals = []\n",
    "    \n",
    "    acc_bkp = 0\n",
    "    acc_mean = np.zeros((n_nodes,1))\n",
    "    for bkpt in range(n_bkps):\n",
    "        # Get new segment mean\n",
    "        subset_freq = np.random.choice(range(n_nodes), size=random_freq, replace=False)\n",
    "        if bkpt == 0:\n",
    "            subset_freq = np.arange(random_freq)\n",
    "        acc_mean[subset_freq] = np.random.uniform(-5.0,5.0,size=(random_freq,1))\n",
    "\n",
    "        # Generate segment\n",
    "        segment_length = int(np.random.exponential(scale=mean_exponential)+30)\n",
    "        acc_bkp += segment_length\n",
    "        signal = np.random.normal(size=(n_nodes,segment_length)) # Ici on devrait avoir uniform et pas normal\n",
    "        signal = H.filter(signal) + G.igft(acc_mean)\n",
    "        # Record bkp position, mean and signal\n",
    "        bkps.append(acc_bkp)\n",
    "        mus.append(np.copy(acc_mean))\n",
    "        signals.append(signal)\n",
    "    signal = np.concatenate(signals, 1)\n",
    "    return G, PSD, bkps, mus, signal\n",
    "\n",
    "# Params\n",
    "n_nodes = 500\n",
    "G, PSD, true_bkps, mus, signal = generate_erdos_renyi(n_nodes)\n",
    "print(f'{len(true_bkps)} changes points')\n",
    "rpt.display(signal[:5].T, true_bkps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSD Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, m, tau, sig_2):\n",
    "    return np.exp( -np.square(x - m * tau) / sig_2)\n",
    "\n",
    "def estimate_PSD_cov(signal, G):\n",
    "    \"\"\"This functions allows to estimate the PSD of the graphs signals using covariance estimation.\n",
    "\n",
    "    Args:\n",
    "        signal: array, shape (n_nodes, T) the stream of graph signals.\n",
    "        G: pygsp.graphs,  the graph\n",
    "\n",
    "    Returns:\n",
    "        PSD : array, shape (n_nodes) : the PSD of the graphs signals \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cov = np.cov(signal)\n",
    "    PSD = np.diagonal(np.conj(G.U).T @ cov @ G.U)\n",
    "    return PSD\n",
    "\n",
    "\n",
    "def estimate_PSD_perraudin(signal, G, M=100, filtering_method='chebyshev', exact_window_norm=False, interp_degree=15 ):\n",
    "    \"\"\"This functions allows to estimate the PSD of the graphs signals using perraudin method.\n",
    "\n",
    "    Args:\n",
    "        signal: array, shape (n_nodes, T) the stream of graph signals.\n",
    "        G: pygsp.graphs,  the graph\n",
    "        M : int, number of filters to use\n",
    "        filtering_method : string, method to use for filtering graphs, either \"exact\" or \"chebyshev\"\n",
    "        window_norm : bool, use exact computation of the window norm\n",
    "        interp_degree: int, polynomial degree for the interpolation \n",
    "\n",
    "    Returns:\n",
    "        PSD : array, shape (n_nodes) : the PSD of the graphs signals \n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    tau = (M+1) * G.lmax / np.square(M)\n",
    "    sig_2 = tau\n",
    "    psd = []\n",
    "    \n",
    "    if not exact_window_norm:\n",
    "        noise = np.random.normal(size=(signal.shape[0], 10))\n",
    "\n",
    "    for m in range(M):\n",
    "        filter = lambda x : gaussian_kernel(x, m, tau, sig_2)\n",
    "        G_filter = pygsp.filters.Filter(G, filter)\n",
    "        \n",
    "        if exact_window_norm:\n",
    "            window_norm = np.sum(np.square(filter(G.e))) + 1e-16\n",
    "        else:\n",
    "            filterned_noise = G_filter.filter(noise, method=filtering_method)\n",
    "            window_norm = np.mean(np.sum(np.square(filterned_noise), axis=0))\n",
    "\n",
    "\n",
    "        filtered_signal = G_filter.filter(signal, method=filtering_method)\n",
    "        filtered_signal_norm = np.mean(np.sum(np.square(filtered_signal), axis=0))\n",
    "        \n",
    "        psd.append(filtered_signal_norm / window_norm)\n",
    "\n",
    "    xs = tau * np.arange(M)\n",
    "\n",
    "    coeff=np.polyfit(xs,np.array(psd),deg=interp_degree)\n",
    "    p = np.poly1d(coeff)\n",
    "\n",
    "    PSD=p(G.e)\n",
    "    return PSD\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare True PSD and approximation\n",
    "w = 30\n",
    "psd_covariance = estimate_PSD_cov(signal[:,:w], G)\n",
    "psd_perraudin = estimate_PSD_perraudin(signal[:,:w], G)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5)) \n",
    "plt.plot(G.e, psd_covariance, label=\"PSD covariance\")\n",
    "plt.plot(G.e, psd_perraudin, label=\"PSD peraudin\")\n",
    "plt.plot(G.e, PSD, label=\"True PSD\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"PSD\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunctionLGS(rpt.base.BaseCost):\n",
    "    model = \"lgs\"\n",
    "    min_size = 2\n",
    "    def __init__(self, psd, lbd):\n",
    "        self.lbd = lbd\n",
    "        self.signal = None\n",
    "        self.psd = psd\n",
    "        \n",
    "    def fit(self, signal):\n",
    "        self.signal = signal\n",
    "        return self\n",
    "        \n",
    "    def error(self, start, end):\n",
    "        y_tilda_bar = np.mean(self.signal[start:end],axis=0)\n",
    "        temp = np.abs(y_tilda_bar) - 0.5*self.lbd*self.psd\n",
    "        mu_tilda_bar = np.sign(y_tilda_bar) * np.maximum(temp,0)\n",
    "        \n",
    "        sub_sig = np.square(self.signal[start:end] - mu_tilda_bar) / self.psd\n",
    "        sub_sig += self.lbd * np.abs(mu_tilda_bar)\n",
    "        \n",
    "        return np.sum(sub_sig)\n",
    "\n",
    "def LGS(G, signal, PSD, d_max=75, lbd=1., verbose=True):\n",
    "    c1, c2 = 500., 100.\n",
    "    T = signal.shape[1]\n",
    "    GFT = signal @ G.U\n",
    "    PSD = PSD\n",
    "    cost = CostFunctionLGS(PSD, lbd).fit(GFT)\n",
    "    algo = rpt.Dynp(custom_cost=cost).fit(GFT)\n",
    "    best_cost = float('inf')\n",
    "    best_bkps = None\n",
    "    loop = tqdm(range(1,d_max+1)) if verbose else range(1,d_max+1)\n",
    "    for d in loop:\n",
    "        bkps = algo.predict(n_bkps=d)\n",
    "        curr_cost = cost.sum_of_costs(bkps) + d * (c1 + c2 * np.log(T/d))\n",
    "        if curr_cost < best_cost:\n",
    "            best_cost = curr_cost\n",
    "            best_bkps = bkps\n",
    "    return best_bkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results using True PSD\n",
    "T = signal.shape[1]\n",
    "d_max = int(T/np.log(T))\n",
    "bkps = LGS(G, signal.T, PSD, d_max=d_max, lbd=10.)\n",
    "print(f'{len(bkps)} changes points')\n",
    "metrics(true_bkps, bkps)\n",
    "rpt.display(signal[:5].T, true_bkps, bkps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results using PSD estimation through covariance estimation\n",
    "T = signal.shape[1]\n",
    "d_max = int(T/np.log(T))\n",
    "bkps = LGS(G, signal.T, psd_covariance, d_max=d_max, lbd=10.)\n",
    "print(f'{len(bkps)} changes points')\n",
    "metrics(true_bkps, bkps)\n",
    "rpt.display(signal[:5].T, true_bkps, bkps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results using PSD estimation with perraudin methods\n",
    "T = signal.shape[1]\n",
    "d_max = int(T/np.log(T))\n",
    "bkps = LGS(G, signal.T, psd_perraudin, d_max=d_max, lbd=10.)\n",
    "print(f'{len(bkps)} changes points')\n",
    "metrics(true_bkps, bkps)\n",
    "rpt.display(signal[:5].T, true_bkps, bkps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VSGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lassoVSGS(signal, PSD, lbd):\n",
    "    \"\"\"\n",
    "    Solves the lasso problem from Algorithm2 VSGS\n",
    "    Solution is computed via a closed form\n",
    "    \"\"\"\n",
    "    emp_mean = np.mean(signal, axis=0)\n",
    "    temp = np.abs(emp_mean) - 0.5*lbd*PSD\n",
    "    mu = np.sign(emp_mean) * np.maximum(temp,0)\n",
    "    return mu\n",
    "\n",
    "class CostFunctionVSGS(rpt.base.BaseCost):\n",
    "    model = \"vsgs\"\n",
    "    min_size = 2\n",
    "    def __init__(self, psd, filter_mask):\n",
    "        self.signal = None\n",
    "        self.filter_mask = filter_mask\n",
    "        self.psd = psd\n",
    "        \n",
    "    def fit(self, signal):\n",
    "        self.signal = signal\n",
    "        return self\n",
    "        \n",
    "    def error(self, start, end):\n",
    "        emp_mean = np.mean(self.signal[start:end],axis=0)\n",
    "        emp_mean *= self.filter_mask\n",
    "        temp = np.square(self.signal[start:end] - emp_mean) / self.psd\n",
    "        return np.sum(temp)\n",
    "\n",
    "def slopeHeuristic(parameters, T):\n",
    "    X = np.zeros((len(parameters), 3))\n",
    "    X[:,0] = parameters[:,2]\n",
    "    X[:,1] = parameters[:,1]\n",
    "    X[:,2] = parameters[:,1] * np.log(T / parameters[:,1])\n",
    "    X /= T\n",
    "    y = parameters[:,3] / T\n",
    "    constants = -2 * LinearRegression().fit(X, y).coef_\n",
    "    return constants.tolist()\n",
    "\n",
    "def VSGS(G, signal, PSD, lambda_set, d_max=75, verbose=True):\n",
    "    GFT = signal @ G.U\n",
    "    PSD = PSD\n",
    "    T = len(signal)\n",
    "    parameters_list = np.zeros((len(lambda_set)*d_max, 4)) # Array storing lbd, Dm, d, cost for each iteration\n",
    "    bkps_list = [] # List storing bkps for each set of parameters\n",
    "    count = 0\n",
    "    loop = tqdm(lambda_set) if verbose else lambda_set\n",
    "    for lbd in loop:\n",
    "        mu = lassoVSGS(GFT, PSD, lbd)\n",
    "        filter_freq = np.nonzero(np.abs(mu))[0] # Frequencies kept by Lasso\n",
    "        filter_mask = (np.abs(mu) > 0) * 1. # Mask = 1 when a frequency is kept\n",
    "        D_m = int(len(filter_freq)) # Number of kept frequencies\n",
    "        cost = CostFunctionVSGS(PSD, filter_mask).fit(GFT)\n",
    "        algo = rpt.Dynp(custom_cost=cost).fit(GFT)\n",
    "        for d in range(1, d_max+1):\n",
    "            bkps = algo.predict(n_bkps=d)\n",
    "            curr_cost = cost.sum_of_costs(bkps)\n",
    "            bkps_list.append(bkps)\n",
    "            parameters_list[count] = [lbd, d, D_m, curr_cost]\n",
    "            count += 1\n",
    "    K1, K2, K3 = slopeHeuristic(parameters_list, T)\n",
    "    # Compute full costs with penalty + constants\n",
    "    final_costs = parameters_list[:,3]\n",
    "    final_costs += K1 * parameters_list[:,2]\n",
    "    final_costs += K2 * parameters_list[:,1]\n",
    "    final_costs += K3 * parameters_list[:,1] * np.log(T / parameters_list[:,1])\n",
    "\n",
    "    best_params_idx = np.argmin(final_costs)\n",
    "    return bkps_list[best_params_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results using True PSD\n",
    "lambda_set = [0,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1.]\n",
    "T = signal.shape[1]\n",
    "d_max = int(T/np.log(T))\n",
    "bkps = VSGS(G, signal.T, PSD, lambda_set, d_max=d_max)\n",
    "print(f'{len(bkps)} changes points')\n",
    "metrics(true_bkps, bkps)\n",
    "rpt.display(signal[:5].T, true_bkps, bkps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results using PSD estimation through covariance estimation\n",
    "lambda_set = [0,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1.]\n",
    "T = signal.shape[1]\n",
    "d_max = int(T/np.log(T))\n",
    "bkps = VSGS(G, signal.T, psd_covariance, lambda_set, d_max=d_max)\n",
    "print(f'{len(bkps)} changes points')\n",
    "metrics(true_bkps, bkps)\n",
    "rpt.display(signal[:5].T, true_bkps, bkps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results using PSD estimation with perraudin methods\n",
    "lambda_set = [0,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1.]\n",
    "T = signal.shape[1]\n",
    "d_max = int(T/np.log(T))\n",
    "bkps = VSGS(G, signal.T, psd_perraudin, lambda_set, d_max=d_max)\n",
    "print(f'{len(bkps)} changes points')\n",
    "metrics(true_bkps, bkps)\n",
    "rpt.display(signal[:5].T, true_bkps, bkps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kernel(signal):\n",
    "    # Slope heuristics to estimate penalty\n",
    "    T, d = signal.shape\n",
    "    k_min, k_max = 10, min(int(0.4*T), T//6)\n",
    "    X = np.arange(k_min,k_max+1)\n",
    "    y = []\n",
    "    algo = rpt.Dynp(model=\"l2\").fit(signal)\n",
    "    cost = rpt.costs.CostL2().fit(signal)\n",
    "    for k in range(k_min,k_max+1):\n",
    "        curr_bkps = algo.predict(n_bkps=k)\n",
    "        y.append(cost.sum_of_costs(curr_bkps))\n",
    "    slope = LinearRegression().fit(np.expand_dims(X,-1), y).coef_[0]\n",
    "    pen = - 2 * slope\n",
    "    \n",
    "    # Apply kernel change point detection\n",
    "    algo = rpt.KernelCPD(kernel=\"linear\").fit(signal)\n",
    "    bkps = algo.predict(pen=pen)\n",
    "    return bkps\n",
    "    \n",
    "# bkps = Kernel(signal.T @ G.U)\n",
    "bkps = Kernel(signal.T)\n",
    "rpt.display(signal[:5].T, true_bkps, bkps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(method='VSGS', dataset='ER', PSD_type='true', n=100):\n",
    "    \"\"\"\n",
    "    Compute mean performance of given setups over n runs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    method: 'VSGS', 'LGS', 'Kernel'\n",
    "    dataset: 'ER', 'BA'\n",
    "    PSD_type: 'true', 'cov', 'perraudin'\n",
    "    \"\"\"\n",
    "    metric_history = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'hausdorff': [],\n",
    "        'randindex': []\n",
    "    }\n",
    "    for _ in tqdm(range(n)):\n",
    "        # Generate Graph\n",
    "        n_nodes = 500\n",
    "        if dataset == 'BA':\n",
    "            G, PSD, true_bkps, mus, signal = generate_barabasi_albert(n_nodes)\n",
    "        elif dataset == 'ER':   \n",
    "            G, PSD, true_bkps, mus, signal = generate_erdos_renyi(n_nodes)\n",
    "\n",
    "        # Get PSD\n",
    "        w = 40\n",
    "        if PSD_type == 'cov':\n",
    "            PSD = estimate_PSD_cov(signal[:,:w], G)\n",
    "        elif PSD_type == 'perraudin':\n",
    "            PSD = estimate_PSD_perraudin(signal[:,:w], G)\n",
    "\n",
    "        # Apply algorithm\n",
    "        T = signal.shape[1]\n",
    "        d_max = min(int(T/np.log(T)), T//6)\n",
    "        if method == 'LGS':\n",
    "            bkps = LGS(G, signal.T, PSD, d_max=d_max, lbd=0.5, verbose=False)\n",
    "            p, r, hausdorff, randindex = metrics(true_bkps, bkps, verbose=False)\n",
    "        elif method == 'VSGS':\n",
    "            lambda_set = [0,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1.]\n",
    "            bkps = VSGS(G, signal.T, PSD, lambda_set, d_max=d_max, verbose=False)\n",
    "            p, r, hausdorff, randindex = metrics(true_bkps, bkps, verbose=False)\n",
    "        elif method == 'Kernel':\n",
    "            bkps = Kernel(signal.T)\n",
    "            p, r, hausdorff, randindex = metrics(true_bkps, bkps, verbose=False)\n",
    "            \n",
    "        # Record metrics\n",
    "        metric_history['precision'].append(p)\n",
    "        metric_history['recall'].append(r)\n",
    "        metric_history['hausdorff'].append(hausdorff)\n",
    "        metric_history['randindex'].append(randindex)\n",
    "    return metric_history\n",
    "\n",
    "def analyse_benchmark(metric_history):\n",
    "    for k,v in metric_history.items():\n",
    "        data = np.array(v)\n",
    "        print(f'{k:10}: mean {np.mean(v):.2f}, std {np.std(v):.3f}, median {np.median(v):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph methods benchmarks\n",
    "parameters = {\n",
    "    'method': ['LGS', 'VSGS'],\n",
    "    'dataset': ['ER', 'BA'],\n",
    "    'PSD_type': ['true', 'cov', 'perraudin']\n",
    "}\n",
    "for method in parameters['method']:\n",
    "    for dataset in parameters['dataset']:\n",
    "        for PSD_type in parameters['PSD_type']:\n",
    "            print(f'method: {method}, dataset: {dataset}, PSD estimation: {PSD_type}')\n",
    "            metric_history = benchmark(method=method, dataset=dataset, PSD_type=PSD_type, n=100)\n",
    "            analyse_benchmark(metric_history)\n",
    "            print(f'-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel methods benchmarks\n",
    "parameters = {\n",
    "    'method': ['Kernel'],\n",
    "    'dataset': ['ER', 'BA'],\n",
    "}\n",
    "\n",
    "for dataset in parameters['dataset']:\n",
    "        print(f'method: linear kernel, dataset: {dataset}')\n",
    "        metric_history = benchmark(method='Kernel', dataset=dataset, n=100)\n",
    "        analyse_benchmark(metric_history)\n",
    "        print(f'-----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadmydata.load_molene_meteo import load_molene_meteo_dataset\n",
    "import geopandas\n",
    "import contextily as cx\n",
    "\n",
    "CRS = \"EPSG:4326\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_df, stations_df, description = load_molene_meteo_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing of temperature dataframe\n",
    "\n",
    "data_df[\"temp\"] = data_df.t - 273.15  # temperature in Celsius\n",
    "\n",
    "temperature_df = data_df.pivot(\n",
    "    index=\"date\", values=\"temp\", columns=\"station_name\"\n",
    ")\n",
    "\n",
    "# Drop NaN values\n",
    "temperature_df = temperature_df.dropna(axis=1)\n",
    "\n",
    "print(f\"The stations with missing values are {', '.join(sorted(list((set(data_df['station_name'])  - set(temperature_df.columns))))).title()}\") \n",
    "\n",
    "temperature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing of stations dataframe\n",
    "\n",
    "stations_df_filtered = stations_df[stations_df['Nom'].isin(list(temperature_df.columns))]\n",
    "\n",
    "# convert pandas df to geopandas df\n",
    "stations_gdf = geopandas.GeoDataFrame(\n",
    "    stations_df_filtered,\n",
    "    geometry=geopandas.points_from_xy(\n",
    "        stations_df_filtered.Longitude, stations_df_filtered.Latitude\n",
    "    ),\n",
    ").set_crs(CRS)\n",
    "\n",
    "stations_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from math import asin, cos, radians, sin, sqrt\n",
    "from pygsp import graphs\n",
    "\n",
    "def get_geodesic_distance(point_1, point_2) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance (in km) between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    https://stackoverflow.com/a/4913653\n",
    "    \"\"\"\n",
    "\n",
    "    lon1, lat1 = point_1\n",
    "    lon2, lat2 = point_2\n",
    "\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute geodedic distances between all stations \n",
    "stations_np = stations_df_filtered[[\"Longitude\", \"Latitude\"]].to_numpy()\n",
    "dist_mat_condensed = pdist(stations_np, metric=get_geodesic_distance) # Distances computed in km\n",
    "\n",
    "# Compute similarity using gaussian smoothing and median heuristic for bandwith parameter\n",
    "bandwidth = np.median(dist_mat_condensed) # median heuristic\n",
    "exp_similarity = np.exp( -(dist_mat_condensed ** 2) / np.square(bandwidth))\n",
    "\n",
    "# Performs dichotomy to find the minimum threshold that keeps the graph connected with an average degree of at least 3\n",
    "low = 0\n",
    "high = 1\n",
    "threshold = 1\n",
    "eps = 1e-5\n",
    "while np.abs(high - low) > eps:\n",
    "    threshold = (high + low) / 2\n",
    "    adjacency_matrix_gaussian = squareform(np.where(exp_similarity > threshold, exp_similarity, 0.0))\n",
    "    G = graphs.Graph(adjacency_matrix_gaussian)\n",
    "    is_connected = G.is_connected(recompute=True)\n",
    "    if is_connected:\n",
    "        low = threshold\n",
    "    else:\n",
    "        high = threshold\n",
    "\n",
    "adjacency_matrix_gaussian = squareform(np.where(exp_similarity > low, exp_similarity, 0.0))\n",
    "G = graphs.Graph(adjacency_matrix_gaussian)\n",
    "is_connected = G.is_connected(recompute=True)\n",
    "\n",
    "print(f\"The minimum threshold so that the graph is connected is {low:.4f}.\")\n",
    "print(f\"The average degree of the graph is {np.mean(G.d):.2f}.\")\n",
    "print(f\"The average weighted degree of the graph is {np.mean(np.sum(adjacency_matrix_gaussian, axis=0)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "ax = stations_gdf.geometry.plot(figsize=(10, 10))\n",
    "cx.add_basemap(ax, crs=stations_gdf.crs.to_string(), zoom=8)\n",
    "ax.set_axis_off()\n",
    "G.set_coordinates(stations_np)\n",
    "G.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_df_np = np.array(temperature_df)\n",
    "rpt.display(temperature_df_np[:,:5], [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PSD\n",
    "w = 80\n",
    "psd_covariance_temp = estimate_PSD_cov(temperature_df_np[:w].T, G)\n",
    "psd_perraudin_temp = estimate_PSD_perraudin(temperature_df_np[:w].T, G)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5)) \n",
    "plt.plot(G.e, psd_covariance_temp, label=\"PSD covariance\")\n",
    "plt.plot(G.e, psd_perraudin_temp, label=\"PSD peraudin\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"PSD\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Covariance PSD\n",
    "T = temperature_df_np.shape[0]\n",
    "d_max = int(T/np.log(T))\n",
    "bkps = LGS(G, temperature_df_np, psd_covariance_temp, d_max=d_max, lbd=10.)\n",
    "print(f'{len(bkps)} changes points')\n",
    "\n",
    "rpt.display(temperature_df_np[:,:5], bkps)\n",
    "plt.xticks(np.arange(0, 31) * 24, np.arange(1, 32))\n",
    "plt.xlabel('Date (-- January 2014)')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Covariance PSD\n",
    "T = temperature_df_np.shape[0]\n",
    "d_max = int(T/np.log(T))\n",
    "bkps = LGS(G, temperature_df_np, psd_perraudin_temp, d_max=d_max, lbd=10.)\n",
    "print(f'{len(bkps)} changes points')\n",
    "\n",
    "rpt.display(temperature_df_np[:,:5], bkps)\n",
    "plt.xticks(np.arange(0, 31) * 24, np.arange(1, 32))\n",
    "plt.xlabel('Date (-- January 2014)')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VSGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results using Covariance PSD\n",
    "lambda_set = [0,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1.]\n",
    "T = temperature_df_np.shape[0]\n",
    "d_max = int(T/np.log(T))\n",
    "bkps = VSGS(G, temperature_df_np, psd_covariance_temp, lambda_set, d_max=d_max)\n",
    "print(f'{len(bkps)} changes points')\n",
    "rpt.display(temperature_df_np[:,:5], bkps)\n",
    "plt.xticks(np.arange(0, 31) * 24, np.arange(1, 32))\n",
    "plt.xlabel('Date (-- January 2014)')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results using Peraudin PSD\n",
    "lambda_set = [0,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1.]\n",
    "T = temperature_df_np.shape[0]\n",
    "d_max = int(T/np.log(T))\n",
    "bkps = VSGS(G, temperature_df_np, psd_perraudin_temp, lambda_set, d_max=d_max)\n",
    "print(f'{len(bkps)} changes points')\n",
    "rpt.display(temperature_df_np[:,:5], bkps)\n",
    "plt.xticks(np.arange(0, 31) * 24, np.arange(1, 32))\n",
    "plt.xlabel('Date (-- January 2014)')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
